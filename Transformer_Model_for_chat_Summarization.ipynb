{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "05014ac7",
      "metadata": {
        "id": "05014ac7"
      },
      "source": [
        "\n",
        "#  Transformer Model for Chat Summarization Using TensorFlow\n",
        "\n",
        "In this Notebook we will explore summarization using the transformer model. We will  implement the full transformer (both encoder and decoder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee0da363",
      "metadata": {
        "id": "ee0da363"
      },
      "source": [
        "<a name='0'></a>\n",
        "## Introduction\n",
        "\n",
        "Summarization is an important task in natural language processing and could be useful for a consumer enterprise. For example, bots can be used to scrape articles, summarize them, and then you can use sentiment analysis to identify the sentiment about certain stocks. Who wants to read an article or a long email today anyway, In this notebookwill learn to:  \n",
        "\n",
        "- Use built-in functions to preprocess your data\n",
        "- Implement DotProductAttention\n",
        "- Implement Causal Attention\n",
        "- Understand how attention works\n",
        "- Build the transformer model\n",
        "- Evaluate your model\n",
        "- Summarize an article\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7b49d856",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7b49d856",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import re\n",
        "\n",
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=70)\n",
        "\n",
        "tf.keras.utils.set_random_seed(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d56fc570",
      "metadata": {
        "id": "d56fc570"
      },
      "source": [
        "\n",
        "## 1 - Import the Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_train_test_data(data_dir):\n",
        "    # Get the train data\n",
        "    train_data = pd.read_json(f\"{data_dir}/train.json\")\n",
        "    train_data.drop(['id'], axis=1, inplace=True)\n",
        "\n",
        "    # Get the test data\n",
        "    test_data = pd.read_json(f\"{data_dir}/test.json\")\n",
        "    test_data.drop(['id'], axis=1, inplace=True)\n",
        "\n",
        "    return train_data, test_data\n",
        "\n"
      ],
      "metadata": {
        "id": "o7U2Kchjx5fI"
      },
      "id": "o7U2Kchjx5fI",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "074bcce3",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "074bcce3",
        "outputId": "78ed36a2-17c8-4361-c55d-fce436783d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dialogue:\n",
            "Lucas: Hey! How was your day?\r\n",
            "Demi: Hey there! \r\n",
            "Demi: It was pretty fine, actually, thank you!\r\n",
            "Demi: I just got promoted! :D\r\n",
            "Lucas: Whoa! Great news!\r\n",
            "Lucas: Congratulations!\r\n",
            "Lucas: Such a success has to be celebrated.\r\n",
            "Demi: I agree! :D\r\n",
            "Demi: Tonight at Death & Co.?\r\n",
            "Lucas: Sure!\r\n",
            "Lucas: See you there at 10pm?\r\n",
            "Demi: Yeah! See you there! :D\n",
            "\n",
            "Summary:\n",
            "Demi got promoted. She will celebrate that with Lucas at Death & Co at 10 pm.\n"
          ]
        }
      ],
      "source": [
        "data_dir = \"/content/\"\n",
        "\n",
        "train_data, test_data = get_train_test_data(data_dir)\n",
        "\n",
        "# Take one example from the dataset and print it\n",
        "example_summary, example_dialogue = train_data.iloc[10]\n",
        "print(f\"Dialogue:\\n{example_dialogue}\")\n",
        "print(f\"\\nSummary:\\n{example_summary}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04210324",
      "metadata": {
        "id": "04210324"
      },
      "source": [
        "\n",
        "## 2 - Preprocess the data\n",
        "\n",
        "First we will do some preprocessing of the data and split it into inputs and outputs. Here we also remove some of the characters that are specific to this dataset and add the `[EOS]` (end of sentence) token to the end. We will also add a `[SOS]` (start of sentence) token to the beginning of the sentences."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess(input_data):\n",
        "    # Define the custom preprocessing function\n",
        "    def preprocess_util(input_data):\n",
        "        # Convert all text to lowercase\n",
        "        lowercase = input_data.lower()\n",
        "        # Remove newlines and double spaces\n",
        "        removed_newlines = re.sub(\"\\n|\\r|\\t\", \" \",  lowercase)\n",
        "        removed_double_spaces = ' '.join(removed_newlines.split(' '))\n",
        "        # Add start of sentence and end of sentence tokens\n",
        "        s = '[SOS] ' + removed_double_spaces + ' [EOS]'\n",
        "        return s\n",
        "\n",
        "    # Apply the preprocessing to the train and test datasets\n",
        "    input_data['summary'] = input_data.apply(lambda row : preprocess_util(row['summary']), axis = 1)\n",
        "    input_data['dialogue'] = input_data.apply(lambda row : preprocess_util(row['dialogue']), axis = 1)\n",
        "\n",
        "    document = input_data['dialogue']\n",
        "    summary = input_data['summary']\n",
        "\n",
        "    return document, summary"
      ],
      "metadata": {
        "id": "iMMy8fqkyUnm"
      },
      "id": "iMMy8fqkyUnm",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9ba397a0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "9ba397a0"
      },
      "outputs": [],
      "source": [
        "document, summary = preprocess(train_data)\n",
        "document_test, summary_test = preprocess(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fe70280",
      "metadata": {
        "id": "0fe70280"
      },
      "source": [
        "Now we perform the standard preprocessing with the tensorflow library. We will need to modify the filters, because we dont want the `[EOS]` tokens to be removed.\n",
        "\n",
        "Then create the vocabulary by combining the data in the documents and the summaries and using `.fit_on_texts()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5dfab3c8",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dfab3c8",
        "outputId": "5a1b7aa4-247c-43a8-bf17-a5f703d2653a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary: 34250\n"
          ]
        }
      ],
      "source": [
        "# The [ and ] from default tokens cannot be removed, because they mark the SOS and EOS token.\n",
        "filters = '!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n'\n",
        "oov_token = '[UNK]'\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token, lower=False)\n",
        "\n",
        "documents_and_summary = pd.concat([document, summary], ignore_index=True)\n",
        "\n",
        "tokenizer.fit_on_texts(documents_and_summary)\n",
        "\n",
        "inputs = tokenizer.texts_to_sequences(document)\n",
        "targets = tokenizer.texts_to_sequences(summary)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(f'Size of vocabulary: {vocab_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "90dab77d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90dab77d",
        "outputId": "0c0581ff-39ce-4147-c7ca-4b0c7375f92c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SOS] neville: hi there, does anyone remember what date i got married on?  don: are you serious?  neville: dead serious. we're on vacation, and tina's mad at me about something. i have a strange suspicion that this might have something to do with our wedding anniversary, but i have nowhere to check.  wyatt: hang on, i'll ask my wife.  don: haha, someone's in a lot of trouble :d  wyatt: september 17. i hope you remember the year ;) [EOS]\n",
            "[7, 3019, 116, 47, 255, 494, 243, 26, 485, 2, 92, 971, 22, 862, 20, 5, 680, 3019, 1569, 680, 223, 22, 1727, 9, 5366, 1096, 23, 24, 35, 110, 2, 17, 6, 1270, 9942, 16, 38, 256, 17, 110, 4, 30, 25, 156, 671, 2083, 18, 2, 17, 3588, 4, 236, 2727, 1223, 22, 63, 193, 29, 1057, 862, 178, 3589, 13, 6, 195, 14, 1580, 82, 2727, 2128, 2213, 2, 207, 5, 243, 3, 213, 8]\n"
          ]
        }
      ],
      "source": [
        "print(document[5])\n",
        "print(inputs[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7341b3f5",
      "metadata": {
        "id": "7341b3f5"
      },
      "source": [
        "Now we can pad the tokenized sequences for the training data.\n",
        "\n",
        "For the purpose of this notebook we need to limit the length of the sequences, as transformers are really big models and are not meant to be trained in such small environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c5846dd5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "c5846dd5"
      },
      "outputs": [],
      "source": [
        "# Limit the size of the input and output data for being able to run it in this environment.\n",
        "encoder_maxlen = 150\n",
        "decoder_maxlen = 50\n",
        "\n",
        "# Pad the sequences.\n",
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')\n",
        "\n",
        "inputs = tf.cast(inputs, dtype=tf.int32)\n",
        "targets = tf.cast(targets, dtype=tf.int32)\n",
        "\n",
        "# Create the final training dataset.\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58b25fb2",
      "metadata": {
        "id": "58b25fb2"
      },
      "source": [
        "\n",
        "## 3 - Positional Encoding\n",
        "\n",
        "In sequence to sequence tasks, the relative order of our data is extremely important to its meaning. When we were training sequential neural networks such as RNNs, we fed our inputs into the network in order. Information about the order of our data was automatically fed into out model. However, when we train a Transformer network using multi-head attention, we feed our data into the model all at once. While this dramatically reduces training time, there is no information about the order of our data. This is where positional encoding is useful.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0e65672c",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "0e65672c"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(positions, d_model):\n",
        "    \"\"\"\n",
        "    Precomputes a matrix with all the positional encodings\n",
        "\n",
        "    Arguments:\n",
        "        positions (int): Maximum number of positions to be encoded\n",
        "        d_model (int): Encoding size\n",
        "\n",
        "    Returns:\n",
        "        pos_encoding (tf.Tensor): A matrix of shape (1, position, d_model) with the positional encodings\n",
        "    \"\"\"\n",
        "\n",
        "    position = np.arange(positions)[:, np.newaxis]\n",
        "    k = np.arange(d_model)[np.newaxis, :]\n",
        "    i = k // 2\n",
        "\n",
        "    # initialize a matrix angle_rads of all the angles\n",
        "    angle_rates = 1 / np.power(10000, (2 * i) / np.float32(d_model))\n",
        "    angle_rads = position * angle_rates\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e1f1063",
      "metadata": {
        "id": "9e1f1063"
      },
      "source": [
        "\n",
        "## 4 - Masking\n",
        "\n",
        "There are two types of masks that are useful when building our Transformer network: the *padding mask* and the *look-ahead mask*. Both help the softmax computation give the appropriate weights to the words in our input sentence.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cfc7471c",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "cfc7471c"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(decoder_token_ids):\n",
        "    \"\"\"\n",
        "    Creates a matrix mask for the padding cells\n",
        "\n",
        "    Arguments:\n",
        "        decoder_token_ids (matrix like): matrix of size (n, m)\n",
        "\n",
        "    Returns:\n",
        "        mask (tf.Tensor): binary tensor of size (n, 1, m)\n",
        "    \"\"\"\n",
        "    seq = 1 - tf.cast(tf.math.equal(decoder_token_ids, 0), tf.float32)\n",
        "\n",
        "    # add extra dimensions to add the padding to the attention logits.\n",
        "    # this will allow for broadcasting later when comparing sequences\n",
        "    return seq[:, tf.newaxis, :]\n",
        "\n",
        "\n",
        "def create_look_ahead_mask(sequence_length):\n",
        "    \"\"\"\n",
        "    Returns a lower triangular matrix filled with ones\n",
        "\n",
        "    Arguments:\n",
        "        sequence_length (int): matrix size\n",
        "\n",
        "    Returns:\n",
        "        mask (tf.Tensor): binary tensor of size (sequence_length, sequence_length)\n",
        "    \"\"\"\n",
        "    mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00b9c92a",
      "metadata": {
        "id": "00b9c92a"
      },
      "source": [
        "\n",
        "## 5 - Encoder\n",
        "\n",
        "\n",
        "The Transformer Encoder layer pairs self-attention and convolutional neural network style of processing to improve the speed of training and passes K and V matrices to the Decoder, which we'll build later in the assignment. In this section of the assignment, we will implement the Encoder by pairing multi-head attention and a feed forward neural network (Figure 2a).\n",
        "\n",
        "* `MultiHeadAttention` you can think of as computing the self-attention several times to detect different features.\n",
        "* Feed forward neural network contains two Dense layers which we'll implement as the function `FullyConnected`\n",
        "\n",
        "Our input sentence first passes through a *multi-head attention layer*, where the encoder looks at other words in the input sentence as it encodes a specific word. The outputs of the multi-head attention layer are then fed to a *feed forward neural network*. The exact same feed forward network is independently applied to each position.\n",
        "   \n",
        "* For the `MultiHeadAttention` layer, you will use the [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention) implemented in Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "c3fd59d0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "c3fd59d0"
      },
      "outputs": [],
      "source": [
        "def FullyConnected(embedding_dim, fully_connected_dim):\n",
        "    \"\"\"\n",
        "    Returns a sequential model consisting of two dense layers. The first dense layer has\n",
        "    fully_connected_dim neurons and is activated by relu. The second dense layer has\n",
        "    embedding_dim and no activation.\n",
        "\n",
        "    Arguments:\n",
        "        embedding_dim (int): output dimension\n",
        "        fully_connected_dim (int): dimension of the hidden layer\n",
        "\n",
        "    Returns:\n",
        "        _ (tf.keras.Model): sequential model\n",
        "    \"\"\"\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(fully_connected_dim, activation='relu'),  # (batch_size, seq_len, d_model)\n",
        "        tf.keras.layers.Dense(embedding_dim)  # (batch_size, seq_len, d_model)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99d7003a",
      "metadata": {
        "id": "99d7003a"
      },
      "source": [
        "\n",
        "### 5.1 Encoder Layer\n",
        "\n",
        "Now you can pair multi-head attention and feed forward neural network together in an encoder layer! You will also use residual connections and layer normalization to help speed up training (Figure 2a).\n",
        "\n",
        "The encoder block performs the following steps:\n",
        "\n",
        "1. It takes the Q, V, K matrices and a boolean mask to a multi-head attention layer. Remember that to compute *self*-attention Q, V and K are the same.\n",
        "2. There is a skip connection to add your original input `x` and the output of the multi-head attention layer.\n",
        "3. After adding the skip connection, the output passes through the first normalization layer.\n",
        "4. Finally, steps 1-3 are repeated but with the feed forward neural network with a dropout layer instead of the multi-head attention layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "51c1452b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "51c1452b"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    The encoder layer is composed by a multi-head self-attention mechanism,\n",
        "    followed by a simple, positionwise fully connected feed-forward network.\n",
        "    This architecture includes a residual connection around each of the two\n",
        "    sub-layers, followed by layer normalization.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim, num_heads, fully_connected_dim,\n",
        "                 dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embedding_dim,\n",
        "            dropout=dropout_rate\n",
        "        )\n",
        "\n",
        "        self.ffn = FullyConnected(\n",
        "            embedding_dim=embedding_dim,\n",
        "            fully_connected_dim=fully_connected_dim\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
        "\n",
        "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the Encoder Layer\n",
        "\n",
        "        Arguments:\n",
        "            x (tf.Tensor): Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
        "            training (bool): Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            mask (tf.Tensor): Boolean mask to ensure that the padding is not\n",
        "                    treated as part of the input\n",
        "        Returns:\n",
        "            encoder_layer_out (tf.Tensor): Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        # calculate self-attention using mha(~1 line).\n",
        "        # Dropout is added by Keras automatically if the dropout parameter is non-zero during training\n",
        "        self_mha_output = self.mha(x, x, x, mask)  # Self attention (batch_size, input_seq_len, fully_connected_dim)\n",
        "\n",
        "        # skip connection\n",
        "        # apply layer normalization on sum of the input and the attention output to get the\n",
        "        # output of the multi-head attention layer\n",
        "        skip_x_attention = self.layernorm1(x + self_mha_output)  # (batch_size, input_seq_len, fully_connected_dim)\n",
        "\n",
        "        # pass the output of the multi-head attention layer through a ffn\n",
        "        ffn_output = self.ffn(skip_x_attention)  # (batch_size, input_seq_len, fully_connected_dim)\n",
        "\n",
        "        # apply dropout layer to ffn output during training\n",
        "        # use `training=training`\n",
        "        ffn_output = self.dropout_ffn(ffn_output, training=training)\n",
        "\n",
        "        # apply layer normalization on sum of the output from multi-head attention (skip connection) and ffn output\n",
        "        # to get the output of the encoder layer\n",
        "        encoder_layer_out = self.layernorm2(skip_x_attention + ffn_output)  # (batch_size, input_seq_len, embedding_dim)\n",
        "\n",
        "        return encoder_layer_out\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e36f13b",
      "metadata": {
        "id": "2e36f13b"
      },
      "source": [
        "\n",
        "### 5.2 - Full Encoder\n",
        "\n",
        "Now we're ready to build the full Transformer Encoder (Figure 2b), where we will embed our input and add the positional encodings you calculated. We will then feed our encoded embeddings to a stack of Encoder layers.\n",
        "\n",
        "\n",
        "The Encoder class  performs the following steps:\n",
        "\n",
        "1. Pass the input through the Embedding layer.\n",
        "2. Scale the embedding by multiplying it by the square root of the embedding dimension.\n",
        "3. Add the position encoding: self.pos_encoding `[:, :seq_len, :]` to the embedding.\n",
        "4. Pass the encoded embedding through a dropout layer\n",
        "5. Pass the output of the dropout layer through the stack of encoding layers using a for loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "d677d14e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "d677d14e"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    The entire Encoder starts by passing the input to an embedding layer\n",
        "    and using positional encoding to then pass the output through a stack of\n",
        "    encoder Layers\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
        "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, self.embedding_dim)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                                self.embedding_dim)\n",
        "\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
        "                                        num_heads=num_heads,\n",
        "                                        fully_connected_dim=fully_connected_dim,\n",
        "                                        dropout_rate=dropout_rate,\n",
        "                                        layernorm_eps=layernorm_eps)\n",
        "                           for _ in range(self.num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the Encoder\n",
        "\n",
        "        Arguments:\n",
        "            x (tf.Tensor): Tensor of shape (batch_size, seq_len, embedding_dim)\n",
        "            training (bool): Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            mask (tf.Tensor): Boolean mask to ensure that the padding is not\n",
        "                    treated as part of the input\n",
        "\n",
        "        Returns:\n",
        "            x (tf.Tensor): Tensor of shape (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # Pass input through the Embedding layer\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, embedding_dim)\n",
        "        # Scale embedding by multiplying it by the square root of the embedding dimension\n",
        "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
        "        # Add the position encoding to embedding\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        # Pass the encoded embedding through a dropout layer\n",
        "        # use `training=training`\n",
        "        x = self.dropout(x, training=training)\n",
        "        # Pass the output through the stack of encoding layers\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c7356fd",
      "metadata": {
        "id": "9c7356fd"
      },
      "source": [
        "\n",
        "## 6 - Decoder\n",
        "\n",
        "The Decoder layer takes the K and V matrices generated by the Encoder and computes the second multi-head attention layer with the Q matrix from the output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 6.1 - Decoder Layer\n",
        "Again, we'll pair multi-head attention with a feed forward neural network, but this time we'll implement two multi-head attention layers. We will also use residual connections and layer normalization to help speed up training\n"
      ],
      "metadata": {
        "id": "LYj3XHUI4GkG"
      },
      "id": "LYj3XHUI4GkG"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d8d3a38d",
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "d8d3a38d"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    The decoder layer is composed by two multi-head attention blocks,\n",
        "    one that takes the new input and uses self-attention, and the other\n",
        "    one that combines it with the output of the encoder, followed by a\n",
        "    fully connected block.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embedding_dim,\n",
        "            dropout=dropout_rate\n",
        "        )\n",
        "\n",
        "        self.mha2 = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embedding_dim,\n",
        "            dropout=dropout_rate\n",
        "        )\n",
        "\n",
        "        self.ffn = FullyConnected(\n",
        "            embedding_dim=embedding_dim,\n",
        "            fully_connected_dim=fully_connected_dim\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
        "\n",
        "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the Decoder Layer\n",
        "\n",
        "        Arguments:\n",
        "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
        "            enc_output (tf.Tensor): Tensor of shape(batch_size, input_seq_len, fully_connected_dim)\n",
        "            training (bool): Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
        "            padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
        "        Returns:\n",
        "            out3 (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
        "            attn_weights_block1 (tf.Tensor): Tensor of shape (batch_size, num_heads, target_seq_len, target_seq_len)\n",
        "            attn_weights_block2 (tf.Tensor): Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # enc_output.shape == (batch_size, input_seq_len, fully_connected_dim)\n",
        "\n",
        "        # BLOCK 1\n",
        "        # calculate self-attention and return attention scores as attn_weights_block1.\n",
        "        # Dropout will be applied during training (~1 line).\n",
        "        mult_attn_out1,attn_weights_block1 = self.mha1(x,x,x,look_ahead_mask,return_attention_scores=True)\n",
        "\n",
        "        # apply layer normalization (layernorm1) to the sum of the attention output and the input (~1 line)\n",
        "        Q1 = self.layernorm1(x + mult_attn_out1)\n",
        "\n",
        "\n",
        "        # BLOCK 2\n",
        "        # calculate self-attention using the Q from the first block and K and V from the encoder output.\n",
        "        # Dropout will be applied during training\n",
        "        # Return attention scores as attn_weights_block2 (~1 line)\n",
        "        mult_attn_out2,attn_weights_block2 = self.mha2(Q1 , enc_output, enc_output , padding_mask,return_attention_scores=True)\n",
        "\n",
        "        # apply layer normalization (layernorm2) to the sum of the attention output and the output of the first block (~1 line)\n",
        "        mult_attn_out2 = self.layernorm1(mult_attn_out2+Q1)\n",
        "\n",
        "        #BLOCK 3\n",
        "        # pass the output of the second block through a ffn\n",
        "        ffn_output = self.ffn(mult_attn_out2)\n",
        "\n",
        "        # apply a dropout layer to the ffn output\n",
        "        ffn_output = self.dropout_ffn(ffn_output,training=training)\n",
        "\n",
        "        # apply layer normalization (layernorm3) to the sum of the ffn output and the output of the second block\n",
        "        out3 = self.layernorm3 (ffn_output+mult_attn_out2)\n",
        "\n",
        "\n",
        "        return out3,attn_weights_block1,attn_weights_block2\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "41686c8b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41686c8b",
        "outputId": "82ea5b42-7ff1-4d37-d10b-e4816665d0d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using embedding_dim=12 and num_heads=16:\n",
            "\n",
            "q has shape:(1, 15, 12)\n",
            "Output of encoder has shape:(1, 7, 8)\n",
            "\n",
            "Output of decoder layer has shape:(1, 15, 12)\n",
            "Att Weights Block 1 has shape:(1, 16, 15, 15)\n",
            "Att Weights Block 2 has shape:(1, 16, 15, 7)\n"
          ]
        }
      ],
      "source": [
        "# Test our function!\n",
        "key_dim = 12\n",
        "n_heads = 16\n",
        "\n",
        "decoderLayer_test = DecoderLayer(embedding_dim=key_dim, num_heads=n_heads, fully_connected_dim=32)\n",
        "\n",
        "q = np.ones((1, 15, key_dim))\n",
        "encoder_test_output = tf.convert_to_tensor(np.random.rand(1, 7, 8))\n",
        "look_ahead_mask = create_look_ahead_mask(q.shape[1])\n",
        "\n",
        "out, attn_w_b1, attn_w_b2 = decoderLayer_test(q, encoder_test_output, False, look_ahead_mask, None)\n",
        "\n",
        "print(f\"Using embedding_dim={key_dim} and num_heads={n_heads}:\\n\")\n",
        "print(f\"q has shape:{q.shape}\")\n",
        "print(f\"Output of encoder has shape:{encoder_test_output.shape}\\n\")\n",
        "\n",
        "print(f\"Output of decoder layer has shape:{out.shape}\")\n",
        "print(f\"Att Weights Block 1 has shape:{attn_w_b1.shape}\")\n",
        "print(f\"Att Weights Block 2 has shape:{attn_w_b2.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66b82ccf",
      "metadata": {
        "id": "66b82ccf"
      },
      "source": [
        "\n",
        "### 6.2 - Full Decoder\n",
        "Time to use our Decoder layer to build a full Transformer Decoder. We will embed our output and add positional encodings. we will then feed our encoded embeddings to a stack of Decoder layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "57dde3be",
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "57dde3be"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    The entire Encoder starts by passing the target input to an embedding layer\n",
        "    and using positional encoding to then pass the output through a stack of\n",
        "    decoder Layers\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size,\n",
        "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, self.embedding_dim)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n",
        "                                        num_heads=num_heads,\n",
        "                                        fully_connected_dim=fully_connected_dim,\n",
        "                                        dropout_rate=dropout_rate,\n",
        "                                        layernorm_eps=layernorm_eps)\n",
        "                           for _ in range(self.num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "        \"\"\"\n",
        "        Forward  pass for the Decoder\n",
        "\n",
        "        Arguments:\n",
        "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
        "            enc_output (tf.Tensor):  Tensor of shape(batch_size, input_seq_len, fully_connected_dim)\n",
        "            training (bool): Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
        "            padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
        "        Returns:\n",
        "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
        "            attention_weights (dict[str: tf.Tensor]): Dictionary of tensors containing all the attention weights\n",
        "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        # create word embeddings\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # scale embeddings by multiplying by the square root of their dimension\n",
        "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
        "\n",
        "        # add positional encodings to word embedding\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        # apply a dropout layer to x\n",
        "\n",
        "        x = self.dropout(x,training=training)\n",
        "\n",
        "        # use a for loop to pass x through a stack of decoder layers and update attention_weights (~4 lines total)\n",
        "        for i in range(self.num_layers):\n",
        "            # pass x and the encoder output through a stack of decoder layers and save the attention weights\n",
        "            # of block 1 and 2 (~1 line)\n",
        "            x, block1, block2 = self.dec_layers[i]( x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            #update attention_weights dictionary with the attention weights of block 1 and block 2\n",
        "            attention_weights['decoder_layer{}_block1_self_att'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2_decenc_att'.format(i+1)] = block2\n",
        "\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, fully_connected_dim)\n",
        "        return x, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "04e877fb",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04e877fb",
        "outputId": "ce73e57a-5c3e-4370-a6bb-3d9b7c57d806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using num_layers=5, embedding_dim=13 and num_heads=17:\n",
            "\n",
            "x has shape:(3, 4)\n",
            "Output of encoder has shape:(3, 7, 9)\n",
            "\n",
            "Output of decoder has shape:(3, 4, 13)\n",
            "\n",
            "Attention weights:\n",
            "decoder_layer1_block1_self_att has shape:(3, 17, 4, 4)\n",
            "decoder_layer1_block2_decenc_att has shape:(3, 17, 4, 7)\n",
            "decoder_layer2_block1_self_att has shape:(3, 17, 4, 4)\n",
            "decoder_layer2_block2_decenc_att has shape:(3, 17, 4, 7)\n",
            "decoder_layer3_block1_self_att has shape:(3, 17, 4, 4)\n",
            "decoder_layer3_block2_decenc_att has shape:(3, 17, 4, 7)\n",
            "decoder_layer4_block1_self_att has shape:(3, 17, 4, 4)\n",
            "decoder_layer4_block2_decenc_att has shape:(3, 17, 4, 7)\n",
            "decoder_layer5_block1_self_att has shape:(3, 17, 4, 4)\n",
            "decoder_layer5_block2_decenc_att has shape:(3, 17, 4, 7)\n"
          ]
        }
      ],
      "source": [
        "# Test our function!\n",
        "n_layers = 5\n",
        "emb_d = 13\n",
        "n_heads = 17\n",
        "fully_connected_dim = 16\n",
        "target_vocab_size = 300\n",
        "maximum_position_encoding = 6\n",
        "\n",
        "x = np.array([[3, 2, 1, 1], [2, 1, 1, 0], [2, 1, 1, 0]])\n",
        "\n",
        "encoder_test_output = tf.convert_to_tensor(np.random.rand(3, 7, 9))\n",
        "\n",
        "look_ahead_mask = create_look_ahead_mask(x.shape[1])\n",
        "\n",
        "decoder_test = Decoder(n_layers, emb_d, n_heads, fully_connected_dim, target_vocab_size,maximum_position_encoding)\n",
        "\n",
        "outd, att_weights = decoder_test(x, encoder_test_output, False, look_ahead_mask, None)\n",
        "\n",
        "print(f\"Using num_layers={n_layers}, embedding_dim={emb_d} and num_heads={n_heads}:\\n\")\n",
        "print(f\"x has shape:{x.shape}\")\n",
        "print(f\"Output of encoder has shape:{encoder_test_output.shape}\\n\")\n",
        "\n",
        "print(f\"Output of decoder has shape:{outd.shape}\\n\")\n",
        "print(\"Attention weights:\")\n",
        "for name, tensor in att_weights.items():\n",
        "    print(f\"{name} has shape:{tensor.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "848ba4b5",
      "metadata": {
        "id": "848ba4b5"
      },
      "source": [
        "\n",
        "## 7 - Transformer\n",
        "\n",
        "    \n",
        "The flow of data through the Transformer Architecture is as follows:\n",
        "* First our input passes through an Encoder, which is just repeated Encoder layers that we implemented:\n",
        "    - embedding and positional encoding of your input\n",
        "    - multi-head attention on your input\n",
        "    - feed forward neural network to help detect features\n",
        "* Then the predicted output passes through a Decoder, consisting of the decoder layers that we implemented:\n",
        "    - embedding and positional encoding of the output\n",
        "    - multi-head attention on your generated output\n",
        "    - multi-head attention with the Q from the first multi-head attention layer and the K and V from the Encoder\n",
        "    - a feed forward neural network to help detect features\n",
        "* Finally, after the Nth Decoder layer, one dense layer and a softmax are applied to generate prediction for the next output in your sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "c9e6cb07",
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "c9e6cb07"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Complete transformer with an Encoder and a Decoder\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
        "               target_vocab_size, max_positional_encoding_input,\n",
        "               max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers=num_layers,\n",
        "                               embedding_dim=embedding_dim,\n",
        "                               num_heads=num_heads,\n",
        "                               fully_connected_dim=fully_connected_dim,\n",
        "                               input_vocab_size=input_vocab_size,\n",
        "                               maximum_position_encoding=max_positional_encoding_input,\n",
        "                               dropout_rate=dropout_rate,\n",
        "                               layernorm_eps=layernorm_eps)\n",
        "\n",
        "        self.decoder = Decoder(num_layers=num_layers,\n",
        "                               embedding_dim=embedding_dim,\n",
        "                               num_heads=num_heads,\n",
        "                               fully_connected_dim=fully_connected_dim,\n",
        "                               target_vocab_size=target_vocab_size,\n",
        "                               maximum_position_encoding=max_positional_encoding_target,\n",
        "                               dropout_rate=dropout_rate,\n",
        "                               layernorm_eps=layernorm_eps)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, input_sentence, output_sentence, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the entire Transformer\n",
        "        Arguments:\n",
        "            input_sentence (tf.Tensor): Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
        "                              An array of the indexes of the words in the input sentence\n",
        "            output_sentence (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
        "                              An array of the indexes of the words in the output sentence\n",
        "            training (bool): Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            enc_padding_mask (tf.Tensor): Boolean mask to ensure that the padding is not\n",
        "                    treated as part of the input\n",
        "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
        "            dec_padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
        "        Returns:\n",
        "            final_output (tf.Tensor): The final output of the model\n",
        "            attention_weights (dict[str: tf.Tensor]): Dictionary of tensors containing all the attention weights for the decoder\n",
        "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # call self.encoder with the appropriate arguments to get the encoder output\n",
        "        enc_output = self.encoder(input_sentence,training,enc_padding_mask)\n",
        "\n",
        "        # call self.decoder with the appropriate arguments to get the decoder output\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, fully_connected_dim)\n",
        "        dec_output, attention_weights = self.decoder(output_sentence, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        # pass decoder output through a linear layer and softmax (~1 line)\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "\n",
        "        return final_output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "3cd93c99",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cd93c99",
        "outputId": "608e1bf6-489d-4486-9303-3e0bee7235e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using num_layers=3, target_vocab_size=350 and num_heads=17:\n",
            "\n",
            "sentence_a has shape:(1, 7)\n",
            "sentence_b has shape:(1, 7)\n",
            "\n",
            "Output of transformer (summary) has shape:(1, 7, 350)\n",
            "\n",
            "Attention weights:\n",
            "decoder_layer1_block1_self_att has shape:(1, 17, 7, 7)\n",
            "decoder_layer1_block2_decenc_att has shape:(1, 17, 7, 7)\n",
            "decoder_layer2_block1_self_att has shape:(1, 17, 7, 7)\n",
            "decoder_layer2_block2_decenc_att has shape:(1, 17, 7, 7)\n",
            "decoder_layer3_block1_self_att has shape:(1, 17, 7, 7)\n",
            "decoder_layer3_block2_decenc_att has shape:(1, 17, 7, 7)\n"
          ]
        }
      ],
      "source": [
        "# Test our function!\n",
        "n_layers = 3\n",
        "emb_d = 13\n",
        "n_heads = 17\n",
        "fully_connected_dim = 8\n",
        "input_vocab_size = 300\n",
        "target_vocab_size = 350\n",
        "max_positional_encoding_input = 12\n",
        "max_positional_encoding_target = 12\n",
        "\n",
        "transformer = Transformer(n_layers,\n",
        "    emb_d,\n",
        "    n_heads,\n",
        "    fully_connected_dim,\n",
        "    input_vocab_size,\n",
        "    target_vocab_size,\n",
        "    max_positional_encoding_input,\n",
        "    max_positional_encoding_target)\n",
        "\n",
        "# 0 is the padding value\n",
        "sentence_a = np.array([[2, 3, 1, 3, 0, 0, 0]])\n",
        "sentence_b = np.array([[1, 3, 4, 0, 0, 0, 0]])\n",
        "\n",
        "enc_padding_mask = create_padding_mask(sentence_a)\n",
        "dec_padding_mask = create_padding_mask(sentence_a)\n",
        "\n",
        "look_ahead_mask = create_look_ahead_mask(sentence_a.shape[1])\n",
        "\n",
        "test_summary, att_weights = transformer(\n",
        "    sentence_a,\n",
        "    sentence_b,\n",
        "    False,\n",
        "    enc_padding_mask,\n",
        "    look_ahead_mask,\n",
        "    dec_padding_mask\n",
        ")\n",
        "\n",
        "print(f\"Using num_layers={n_layers}, target_vocab_size={target_vocab_size} and num_heads={n_heads}:\\n\")\n",
        "print(f\"sentence_a has shape:{sentence_a.shape}\")\n",
        "print(f\"sentence_b has shape:{sentence_b.shape}\")\n",
        "\n",
        "print(f\"\\nOutput of transformer (summary) has shape:{test_summary.shape}\\n\")\n",
        "print(\"Attention weights:\")\n",
        "for name, tensor in att_weights.items():\n",
        "    print(f\"{name} has shape:{tensor.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33e8a0c2",
      "metadata": {
        "id": "33e8a0c2"
      },
      "source": [
        "\n",
        "## 8 - Initialize the Model\n",
        "Now that we have defined the model, we can initialize and train it. First we can initialize the model with the parameters below. Note that generally these models are much larger and we are using a smaller version to fit this environment and to be able to train it in just a few minutes.\n",
        "\n",
        "The base model described in the original Transformer paper used `num_layers=6`, `embedding_dim=512`, and `fully_connected_dim=2048`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "a5f79f64",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "a5f79f64"
      },
      "outputs": [],
      "source": [
        "# Define the model parameters\n",
        "num_layers = 2\n",
        "embedding_dim = 128\n",
        "fully_connected_dim = 128\n",
        "num_heads = 2\n",
        "positional_encoding_length = 256\n",
        "\n",
        "# Initialize the model\n",
        "transformer = Transformer(\n",
        "    num_layers,\n",
        "    embedding_dim,\n",
        "    num_heads,\n",
        "    fully_connected_dim,\n",
        "    vocab_size,\n",
        "    vocab_size,\n",
        "    positional_encoding_length,\n",
        "    positional_encoding_length,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71473c27",
      "metadata": {
        "id": "71473c27"
      },
      "source": [
        "\n",
        "## 9 - Prepare for Training the Model\n",
        "\n",
        "The original transformer paper uses Adam optimizer with custom learning rate scheduling, which we define in the cell below. This was empirically shown to produce faster convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "eb402089",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "eb402089"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.d_model = tf.cast(d_model, dtype=tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, dtype=tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(embedding_dim)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad854ab6",
      "metadata": {
        "id": "ad854ab6"
      },
      "source": [
        "Below you can plot, how the custom learning rate looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "35a17a59",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "35a17a59",
        "outputId": "71df98c4-72d7-422c-85ee-c0881e91d6f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.xlabel('Train Step')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cfba386",
      "metadata": {
        "id": "4cfba386"
      },
      "source": [
        "Next, we set up the loss. Since the target sequences are padded, it is important to apply a padding mask when calculating the loss.\n",
        "\n",
        "We will use the sparse categorical cross-entropy loss function (`tf.keras.losses.SparseCategoricalCrossentropy`) and set the parameter `from_logits` to False since the Transformer does not output raw logits since the last layer has a softmax activation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "99fc8885",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "99fc8885"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
        "\n",
        "def masked_loss(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "\n",
        "# Here we will store the losses, so we can later plot them\n",
        "losses = []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33db3f0b",
      "metadata": {
        "id": "33db3f0b"
      },
      "source": [
        "Now we can define our custom training function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "79092091",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "79092091"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(model, inp, tar):\n",
        "    \"\"\"\n",
        "    One training step for the transformer\n",
        "    Arguments:\n",
        "        inp (tf.Tensor): Input data to summarize\n",
        "        tar (tf.Tensor): Target (summary)\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    # Create masks\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar_inp)[1])\n",
        "    dec_padding_mask = create_padding_mask(inp) # Notice that both encoder and decoder padding masks are equal\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = model(\n",
        "            inp,\n",
        "            tar_inp,\n",
        "            True,\n",
        "            enc_padding_mask,\n",
        "            look_ahead_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = masked_loss(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1480d5fd",
      "metadata": {
        "id": "1480d5fd"
      },
      "source": [
        "Now you are ready for training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79e05c54",
      "metadata": {
        "id": "79e05c54"
      },
      "source": [
        "\n",
        "## 10 - Summarization\n",
        "\n",
        "The last thing we will implement is inference. With this, we will be able to produce actual summaries of the documents. We will use a simple method called greedy decoding, which means we will predict one word at a time and append it to the output. We will start with an `[SOS]` token and repeat the word by word inference until the model returns us the `[EOS]` token or until we reach the maximum length of the sentence (we need to add this limit, otherwise a poorly trained model could give you infinite sentences without ever producing the `[EOS]` token.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "175fae70",
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "175fae70"
      },
      "outputs": [],
      "source": [
        "\n",
        "def next_word(model, encoder_input, output):\n",
        "    \"\"\"\n",
        "    Helper function for summarization that uses the model to predict just the next word.\n",
        "    Arguments:\n",
        "        encoder_input (tf.Tensor): Input data to summarize\n",
        "        output (tf.Tensor): (incomplete) target (summary)\n",
        "    Returns:\n",
        "        predicted_id (tf.Tensor): The id of the predicted word\n",
        "    \"\"\"\n",
        "    # Create a padding mask for the input (encoder)\n",
        "    enc_padding_mask = create_padding_mask(encoder_input)\n",
        "    # Create a look-ahead mask for the output\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(output)[1])\n",
        "    # Create a padding mask for the input (decoder)\n",
        "    dec_padding_mask = create_padding_mask(encoder_input)\n",
        "\n",
        "    # Run the prediction of the next word with the transformer model\n",
        "    predictions, attention_weights = model(encoder_input, output, False,\n",
        "                                           enc_padding_mask, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "    # Select the last word predictions\n",
        "    predictions = predictions[:, -1:, :]  # Get the predictions for the last token\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    return predicted_id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29af50d0",
      "metadata": {
        "id": "29af50d0"
      },
      "source": [
        "Check if the function works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "3e97ba77",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e97ba77",
        "outputId": "5e190e36-3e99-46ab-84d5-a3f546d9fa13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted token: [[30832]]\n",
            "Predicted word: hahahaha😂😂😂😂😂😂😂\n"
          ]
        }
      ],
      "source": [
        "# Take a random sentence as an input\n",
        "input_document = tokenizer.texts_to_sequences([\"a random sentence\"])\n",
        "input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "# Take the start of sentence token as the only token in the output to predict the next word\n",
        "output = tf.expand_dims([tokenizer.word_index[\"[SOS]\"]], 0)\n",
        "\n",
        "# predict the next word with your function\n",
        "predicted_token = next_word(transformer, encoder_input, output)\n",
        "print(f\"Predicted token: {predicted_token}\")\n",
        "\n",
        "predicted_word = tokenizer.sequences_to_texts(predicted_token.numpy())[0]\n",
        "print(f\"Predicted word: {predicted_word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "6177dc6a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "id": "6177dc6a"
      },
      "outputs": [],
      "source": [
        "def summarize(model, input_document):\n",
        "    \"\"\"\n",
        "    A function for summarization using the transformer model\n",
        "    Arguments:\n",
        "        input_document (tf.Tensor): Input data to summarize\n",
        "    Returns:\n",
        "        _ (str): The summary of the input_document\n",
        "    \"\"\"\n",
        "    input_document = tokenizer.texts_to_sequences([input_document])\n",
        "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "    output = tf.expand_dims([tokenizer.word_index[\"[SOS]\"]], 0)\n",
        "\n",
        "    for i in range(decoder_maxlen):\n",
        "        predicted_id = next_word(model, encoder_input, output)\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "        if predicted_id == tokenizer.word_index[\"[EOS]\"]:\n",
        "            break\n",
        "\n",
        "    return tokenizer.sequences_to_texts(output.numpy())[0]  # since there is just one translated document"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3b15117",
      "metadata": {
        "id": "d3b15117"
      },
      "source": [
        "Now we can already summarize a sentence! But beware, since the model was not yet trained at all, it will just produce nonsense."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "bae4d5f1",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "bae4d5f1",
        "outputId": "b5129b2d-8c5d-4a42-9e28-fbeab48e15a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set example:\n",
            "[SOS] amanda: i baked  cookies. do you want some?  jerry: sure!  amanda: i'll bring you tomorrow :-) [EOS]\n",
            "\n",
            "Human written summary:\n",
            "[SOS] amanda baked cookies and will bring jerry some tomorrow. [EOS]\n",
            "\n",
            "Model written summary:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[SOS] hahahaha😂😂😂😂😂😂😂 recheck messing messing amanda bettina affected die amanda tomasz electoral pranksters time… hahhah 👍👍20th moaning taht coctails helicoptered wild' mock venezia 👱\\u200d♀️ taht north north wrapped wrapped wrapped maybelline fluffly clarissa's developments developments developments developments offering developments tvs developments haemorrhoids maybelline 00pm spuds denon elary sorbonne carey's pressing sauté\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "training_set_example = 0\n",
        "\n",
        "# Check a summary of a document from the training set\n",
        "print('Training set example:')\n",
        "print(document[training_set_example])\n",
        "print('\\nHuman written summary:')\n",
        "print(summary[training_set_example])\n",
        "print('\\nModel written summary:')\n",
        "summarize(transformer, document[training_set_example])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90d6f836",
      "metadata": {
        "id": "90d6f836"
      },
      "source": [
        "\n",
        "## 11 - Train the model\n",
        "\n",
        "Now we can finally train the model. Below is a loop that will train our model for 25 epochs. note that it should take about 30 seconds per epoch (with the exception of the first few epochs which can take a few minutes each).\n",
        "\n",
        "Note that after each epoch we perform the summarization on one of the sentences in the test set and print it out, so we can see how our model is improving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "ebe2bf5f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebe2bf5f",
        "outputId": "9965be0d-3940-411e-ea7e-17a204458cb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss 7.8950\n",
            "Time taken for one epoch: 63.40859389305115 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] [EOS]\n",
            "\n",
            "Epoch 2, Loss 6.5900\n",
            "Time taken for one epoch: 29.199660778045654 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] is going to the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            "\n",
            "Epoch 3, Loss 6.0242\n",
            "Time taken for one epoch: 25.6218740940094 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] tom is going to the new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new\n",
            "\n",
            "Epoch 4, Loss 5.6729\n",
            "Time taken for one epoch: 21.836296558380127 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] john is going to the new new new new new new new new new new new new new new new new new new new new new new week [EOS]\n",
            "\n",
            "Epoch 5, Loss 5.4614\n",
            "Time taken for one epoch: 22.53374981880188 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] tom is going to the new new new new job [EOS]\n",
            "\n",
            "Epoch 6, Loss 5.3072\n",
            "Time taken for one epoch: 23.009347438812256 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] tom is going to the new job she will be there [EOS]\n",
            "\n",
            "Epoch 7, Loss 5.1817\n",
            "Time taken for one epoch: 22.55953621864319 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] tom is going to the party with her birthday party [EOS]\n",
            "\n",
            "Epoch 8, Loss 5.0700\n",
            "Time taken for one epoch: 21.760409593582153 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] the new job is going to the party [EOS]\n",
            "\n",
            "Epoch 9, Loss 4.9669\n",
            "Time taken for one epoch: 21.60362672805786 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] tom is going to the party with her birthday party [EOS]\n",
            "\n",
            "Epoch 10, Loss 4.8681\n",
            "Time taken for one epoch: 21.423439979553223 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] kate is going to the party because he has to be there [EOS]\n",
            "\n",
            "Epoch 11, Loss 4.7682\n",
            "Time taken for one epoch: 21.72010636329651 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] sam is going to go to the party with her [EOS]\n",
            "\n",
            "Epoch 12, Loss 4.6691\n",
            "Time taken for one epoch: 21.45612096786499 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] anna has a lot of work to be there on the office [EOS]\n",
            "\n",
            "Epoch 13, Loss 4.5687\n",
            "Time taken for one epoch: 21.404280424118042 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] sam has a date with her mother she has to come to her [EOS]\n",
            "\n",
            "Epoch 14, Loss 4.4714\n",
            "Time taken for one epoch: 21.4178524017334 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] sam has a date with her mother she has to go to the party [EOS]\n",
            "\n",
            "Epoch 15, Loss 4.3702\n",
            "Time taken for one epoch: 21.323112964630127 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] hannah has a date with his phone and her to do it [EOS]\n",
            "\n",
            "Epoch 16, Loss 4.2723\n",
            "Time taken for one epoch: 21.47480869293213 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] hannah has a problem with her phone and her phone he has to work [EOS]\n",
            "\n",
            "Epoch 17, Loss 4.1719\n",
            "Time taken for one epoch: 21.822381734848022 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] hannah has a problem with his mother she has to talk to her [EOS]\n",
            "\n",
            "Epoch 18, Loss 4.0752\n",
            "Time taken for one epoch: 21.39966607093811 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] hannah has to talk about the phone and he has to do it [EOS]\n",
            "\n",
            "Epoch 19, Loss 3.9757\n",
            "Time taken for one epoch: 21.382249116897583 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] hannah has just finished his phone and she has to do it again and she will be at the moment [EOS]\n",
            "\n",
            "Epoch 20, Loss 3.8848\n",
            "Time taken for one epoch: 21.215116024017334 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] hannah has a problem with his number to her [EOS]\n",
            "\n",
            "Epoch 21, Loss 3.7891\n",
            "Time taken for one epoch: 21.403441667556763 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] hannah has just finished his phone and hannah can't find it again [EOS]\n",
            "\n",
            "Epoch 22, Loss 3.6951\n",
            "Time taken for one epoch: 21.317484378814697 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] hannah has a problem with her number from her [EOS]\n",
            "\n",
            "Epoch 23, Loss 3.6033\n",
            "Time taken for one epoch: 21.417513132095337 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] hannah has a problem with her laptop for a walk and she will be at the park [EOS]\n",
            "\n",
            "Epoch 24, Loss 3.5144\n",
            "Time taken for one epoch: 21.210519790649414 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] hannah has just finished his number of her desk to do it again amanda is going to be at the park [EOS]\n",
            "\n",
            "Epoch 25, Loss 3.4272\n",
            "Time taken for one epoch: 21.499215602874756 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] hannah has just finished his number of her phone and hannah are going to get him a lift [EOS]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take an example from the test set, to monitor it during training\n",
        "test_example = 0\n",
        "true_summary = summary_test[test_example]\n",
        "true_document = document_test[test_example]\n",
        "\n",
        "# Define the number of epochs\n",
        "epochs = 25\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    start = time.time()\n",
        "    train_loss.reset_states()\n",
        "    number_of_batches=len(list(enumerate(dataset)))\n",
        "\n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        print(f'Epoch {epoch+1}, Batch {batch+1}/{number_of_batches}', end='\\r')\n",
        "        train_step(transformer, inp, tar)\n",
        "\n",
        "    print (f'Epoch {epoch+1}, Loss {train_loss.result():.4f}')\n",
        "    losses.append(train_loss.result())\n",
        "\n",
        "    print (f'Time taken for one epoch: {time.time() - start} sec')\n",
        "    print('Example summarization on the test set:')\n",
        "    print('  True summarization:')\n",
        "    print(f'    {true_summary}')\n",
        "    print('  Predicted summarization:')\n",
        "    print(f'    {summarize(transformer, true_document)}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35687ddc",
      "metadata": {
        "id": "35687ddc"
      },
      "source": [
        "Plot the loss funtion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "eb3d5335",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "eb3d5335",
        "outputId": "1b9c61ee-8215-4edb-e7b2-0567ff27fc8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Epoch')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(losses)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6a53f16",
      "metadata": {
        "id": "b6a53f16"
      },
      "source": [
        "\n",
        "## 12 - Summarize some Sentences!\n",
        "\n",
        "Below we can see an example of summarization of a sentence from the training set and a sentence from the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "2493b755",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2493b755",
        "outputId": "08d5a104-6dda-4298-eaa3-a5a4a20417db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set example:\n",
            "[SOS] amanda: i baked  cookies. do you want some?  jerry: sure!  amanda: i'll bring you tomorrow :-) [EOS]\n",
            "\n",
            "Human written summary:\n",
            "[SOS] amanda baked cookies and will bring jerry some tomorrow. [EOS]\n",
            "\n",
            "Model written summary:\n",
            "[SOS] amanda will bring some cookies tomorrow [EOS]\n"
          ]
        }
      ],
      "source": [
        "training_set_example = 0\n",
        "\n",
        "# Check a summary of a document from the training set\n",
        "print('Training set example:')\n",
        "print(document[training_set_example])\n",
        "print('\\nHuman written summary:')\n",
        "print(summary[training_set_example])\n",
        "print('\\nModel written summary:')\n",
        "print(summarize(transformer, document[training_set_example]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "15baaa47",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "graded"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15baaa47",
        "outputId": "605e5b4e-50fd-4e61-eaf7-92f126a5b86b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set example:\n",
            "[SOS] ollie: hi , are you in warsaw  jane: yes, just back! btw are you free for diner the 19th?  ollie: nope!  jane: and the  18th?  ollie: nope, we have this party and you must be there, remember?  jane: oh right! i lost my calendar..  thanks for reminding me  ollie: we have lunch this week?  jane: with pleasure!  ollie: friday?  jane: ok  jane: what do you mean \" we don't have any more whisky!\" lol..  ollie: what!!!  jane: you just call me and the all thing i heard was that sentence about whisky... what's wrong with you?  ollie: oh oh... very strange! i have to be carefull may be there is some spy in my mobile! lol  jane: dont' worry, we'll check on friday.  ollie: don't forget to bring some sun with you  jane: i can't wait to be in morocco..  ollie: enjoy and see you friday  jane: sorry ollie, i'm very busy, i won't have time for lunch  tomorrow, but may be at 6pm after my courses?this trip to morocco was so nice, but time consuming!  ollie: ok for tea!  jane: i'm on my way..  ollie: tea is ready, did you bring the pastries?  jane: i already ate them all... see you in a minute  ollie: ok [EOS]\n",
            "\n",
            "Human written summary:\n",
            "[SOS] jane is in warsaw. ollie and jane has a party. jane lost her calendar. they will get a lunch this week on friday. ollie accidentally called jane and talked about whisky. jane cancels lunch. they'll meet for a tea at 6 pm. [EOS]\n",
            "\n",
            "Model written summary:\n",
            "[SOS] jane is going to warsaw for her birthday party on friday and jane will be there for friday in the afternoon [EOS]\n"
          ]
        }
      ],
      "source": [
        "test_set_example = 4\n",
        "\n",
        "# Check a summary of a document from the test set\n",
        "print('Test set example:')\n",
        "print(document_test[test_set_example])\n",
        "print('\\nHuman written summary:')\n",
        "print(summary_test[test_set_example])\n",
        "print('\\nModel written summary:')\n",
        "print(summarize(transformer, document_test[test_set_example]))"
      ]
    }
  ],
  "metadata": {
    "grader_version": "1",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}